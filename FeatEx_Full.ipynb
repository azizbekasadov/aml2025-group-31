{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qb_DSGmfwcd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install librosa tensorflow-hub panns-inference kagglehub tqdm\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import torch\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import kagglehub\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkzsO4kCwb2l",
        "outputId": "9c764839-7b86-4abe-cdf7-994afa8f6f4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.11/dist-packages (0.16.1)\n",
            "Collecting panns-inference\n",
            "  Downloading panns_inference-0.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub) (5.29.5)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub) (2.18.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from panns-inference) (3.10.0)\n",
            "Collecting torchlibrosa (from panns-inference)\n",
            "  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tf-keras>=2.14.1->tensorflow-hub) (2.18.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns-inference) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns-inference) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns-inference) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns-inference) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns-inference) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns-inference) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns-inference) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->panns-inference) (1.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (1.73.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras>=2.14.1->tensorflow-hub) (0.1.2)\n",
            "Downloading panns_inference-0.1.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: torchlibrosa, panns-inference\n",
            "Successfully installed panns-inference-0.1.1 torchlibrosa-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "print(\"GPU Available:\", tf.config.experimental.list_physical_devices('GPU'))\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3Z9vBdNwntB",
        "outputId": "cf5f0eaf-0549-41b8-a780-ab0f7235114b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "CUDA Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 2: Download Dataset\n",
        "# ==========================================\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "dataset_path = kagglehub.dataset_download(\"murtadhanajim/gender-recognition-by-voiceoriginal\")\n",
        "print(f\"Dataset downloaded to: {dataset_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTvJP9RdwqQp",
        "outputId": "529a5c0a-0ee7-4035-be8d-e6a410c20164"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset...\n",
            "Dataset downloaded to: /kaggle/input/gender-recognition-by-voiceoriginal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 3: Model Loading Functions\n",
        "# ==========================================\n",
        "\n",
        "class AudioFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the three pre-trained models exactly as described in the paper\"\"\"\n",
        "        print(\"Loading pre-trained models...\")\n",
        "\n",
        "        # 1. VGGish Model (128-dimensional features)\n",
        "        print(\"Loading VGGish...\")\n",
        "        self.vggish_model = hub.load('https://tfhub.dev/google/vggish/1')\n",
        "\n",
        "        # 2. YAMNet Model (1024-dimensional features)\n",
        "        print(\"Loading YAMNet...\")\n",
        "        self.yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
        "\n",
        "        # 3. PANNs Model (2048-dimensional features)\n",
        "        print(\"Setting up PANNs...\")\n",
        "        self.setup_panns()\n",
        "\n",
        "        print(\"All models loaded successfully!\")\n",
        "\n",
        "    def setup_panns(self):\n",
        "        \"\"\"Setup PANNs model\"\"\"\n",
        "        try:\n",
        "            from panns_inference import AudioTagging\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            self.panns_model = AudioTagging(checkpoint_path=None, device=device)\n",
        "            print(f\"PANNs loaded on {device}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading PANNs: {e}\")\n",
        "            print(\"Installing panns_inference...\")\n",
        "            !pip install panns_inference\n",
        "            from panns_inference import AudioTagging\n",
        "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "            self.panns_model = AudioTagging(checkpoint_path=None, device=device)\n",
        "\n",
        "    def load_audio(self, file_path, target_sr=16000):\n",
        "        \"\"\"Load and preprocess audio file\"\"\"\n",
        "        try:\n",
        "            audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
        "            return audio, sr\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "    def extract_vggish_features(self, audio, sr=16000):\n",
        "        \"\"\"\n",
        "        Extract VGGish features (128-dimensional)\n",
        "        Paper: \"VGGish model generates a 128-dimensional deep feature vector for every 0.96 s\"\n",
        "        \"we averaged those 128-dimensional VGGish-based deep features\"\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if len(audio) == 0:\n",
        "                return np.zeros(128)\n",
        "\n",
        "            # VGGish expects float32 tensor\n",
        "            audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
        "\n",
        "            # Extract embeddings - returns features for each 0.96s segment\n",
        "            embeddings = self.vggish_model(audio_tensor)\n",
        "\n",
        "            # Average across time dimension as described in paper\n",
        "            if len(embeddings.shape) > 1:\n",
        "                features = tf.reduce_mean(embeddings, axis=0)\n",
        "            else:\n",
        "                features = embeddings\n",
        "\n",
        "            return features.numpy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"VGGish extraction error: {e}\")\n",
        "            return np.zeros(128)\n",
        "\n",
        "    def extract_yamnet_features(self, audio, sr=16000):\n",
        "        \"\"\"\n",
        "        Extract YAMNet features (1024-dimensional)\n",
        "        Paper: \"YAMNet produces a 1024-dimensional deep feature vector for every 0.48 s\"\n",
        "        \"we averaged those 1024-dimensional YAMNet-based deep features\"\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if len(audio) == 0:\n",
        "                return np.zeros(1024)\n",
        "\n",
        "            audio_tensor = tf.convert_to_tensor(audio, dtype=tf.float32)\n",
        "\n",
        "            # Get YAMNet outputs: scores, embeddings, spectrogram\n",
        "            scores, embeddings, spectrogram = self.yamnet_model(audio_tensor)\n",
        "\n",
        "            # Average embeddings across time as described in paper\n",
        "            if len(embeddings.shape) > 1:\n",
        "                features = tf.reduce_mean(embeddings, axis=0)\n",
        "            else:\n",
        "                features = embeddings\n",
        "\n",
        "            return features.numpy()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"YAMNet extraction error: {e}\")\n",
        "            return np.zeros(1024)\n",
        "\n",
        "    def extract_panns_features(self, audio, sr=16000):\n",
        "        \"\"\"\n",
        "        Extract PANNs features (2048-dimensional)\n",
        "        Paper: \"PANNs are pre-trained models specifically developed for audio pattern recognition\"\n",
        "        \"These audio patterns are then mapped to a 2048-dimensional output space\"\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if len(audio) == 0:\n",
        "                return np.zeros(2048)\n",
        "\n",
        "            # PANNs typically expects 32kHz audio\n",
        "            if sr != 32000:\n",
        "                audio_32k = librosa.resample(audio, orig_sr=sr, target_sr=32000)\n",
        "            else:\n",
        "                audio_32k = audio\n",
        "\n",
        "            # Get PANNs inference\n",
        "            (clipwise_output, embedding) = self.panns_model.inference(audio_32k[None, :])\n",
        "\n",
        "            # Ensure 2048 dimensions as specified in paper\n",
        "            if embedding.shape[1] == 2048:\n",
        "                return embedding[0]\n",
        "            elif embedding.shape[1] > 2048:\n",
        "                return embedding[0][:2048]  # Truncate if larger\n",
        "            else:\n",
        "                # Pad if smaller\n",
        "                padded = np.zeros(2048)\n",
        "                padded[:embedding.shape[1]] = embedding[0]\n",
        "                return padded\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"PANNs extraction error: {e}\")\n",
        "            return np.zeros(2048)\n",
        "\n",
        "    def extract_all_features(self, file_path):\n",
        "        \"\"\"Extract features from all three models for a single audio file\"\"\"\n",
        "        # Load audio at 16kHz (standard for these models)\n",
        "        audio, sr = self.load_audio(file_path, target_sr=16000)\n",
        "\n",
        "        if audio is None:\n",
        "            return {\n",
        "                'vggish': np.zeros(128),\n",
        "                'yamnet': np.zeros(1024),\n",
        "                'panns': np.zeros(2048)\n",
        "            }\n",
        "\n",
        "        # Extract features from each model\n",
        "        features = {\n",
        "            'vggish': self.extract_vggish_features(audio, sr),\n",
        "            'yamnet': self.extract_yamnet_features(audio, sr),\n",
        "            'panns': self.extract_panns_features(audio, sr)\n",
        "        }\n",
        "\n",
        "        return features"
      ],
      "metadata": {
        "id": "Pyo5JAsWwwoa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 4: Dataset Processing Function\n",
        "# ==========================================\n",
        "\n",
        "def process_gender_dataset(dataset_path, output_dir='./audio_features'):\n",
        "    \"\"\"Process the gender recognition dataset and extract features\"\"\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Initialize extractor\n",
        "    print(\"Initializing feature extractor...\")\n",
        "    extractor = AudioFeatureExtractor()\n",
        "\n",
        "    # Find all audio files\n",
        "    audio_extensions = ['.wav', '.mp3', '.flac', '.m4a', '.ogg']\n",
        "    audio_files = []\n",
        "\n",
        "    print(\"Scanning for audio files...\")\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "        for file in files:\n",
        "            if any(file.lower().endswith(ext) for ext in audio_extensions):\n",
        "                audio_files.append(os.path.join(root, file))\n",
        "\n",
        "    print(f\"Found {len(audio_files)} audio files\")\n",
        "\n",
        "    if len(audio_files) == 0:\n",
        "        print(\"No audio files found! Please check the dataset path.\")\n",
        "        return None, None\n",
        "\n",
        "    # Initialize feature storage\n",
        "    features_dict = {\n",
        "        'vggish': [],\n",
        "        'yamnet': [],\n",
        "        'panns': []\n",
        "    }\n",
        "\n",
        "    file_metadata = []\n",
        "\n",
        "    # Process each file\n",
        "    print(\"Extracting features...\")\n",
        "    for i, file_path in enumerate(tqdm(audio_files, desc=\"Processing audio files\")):\n",
        "        try:\n",
        "            # Extract features\n",
        "            features = extractor.extract_all_features(file_path)\n",
        "\n",
        "            # Store features\n",
        "            features_dict['vggish'].append(features['vggish'])\n",
        "            features_dict['yamnet'].append(features['yamnet'])\n",
        "            features_dict['panns'].append(features['panns'])\n",
        "\n",
        "            # Extract metadata from filename (assuming gender is encoded in filename)\n",
        "            filename = os.path.basename(file_path)\n",
        "\n",
        "            # Try to extract gender from filename (adapt this based on your dataset structure)\n",
        "            gender = 'unknown'  # Default\n",
        "            if 'male' in filename.lower() or 'm_' in filename.lower():\n",
        "                gender = 'male'\n",
        "            elif 'female' in filename.lower() or 'f_' in filename.lower():\n",
        "                gender = 'female'\n",
        "\n",
        "            file_metadata.append({\n",
        "                'index': i,\n",
        "                'filename': filename,\n",
        "                'file_path': file_path,\n",
        "                'gender': gender\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            # Add zero features for failed files\n",
        "            features_dict['vggish'].append(np.zeros(128))\n",
        "            features_dict['yamnet'].append(np.zeros(1024))\n",
        "            features_dict['panns'].append(np.zeros(2048))\n",
        "\n",
        "            file_metadata.append({\n",
        "                'index': i,\n",
        "                'filename': os.path.basename(file_path),\n",
        "                'file_path': file_path,\n",
        "                'gender': 'unknown'\n",
        "            })\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    for model_name in features_dict:\n",
        "        features_dict[model_name] = np.array(features_dict[model_name])\n",
        "        print(f\"{model_name.upper()} features shape: {features_dict[model_name].shape}\")\n",
        "\n",
        "    # Save features\n",
        "    print(\"Saving extracted features...\")\n",
        "    for model_name, features_array in features_dict.items():\n",
        "        np.save(os.path.join(output_dir, f'{model_name}_features.npy'), features_array)\n",
        "\n",
        "    # Save metadata\n",
        "    metadata_df = pd.DataFrame(file_metadata)\n",
        "    metadata_df.to_csv(os.path.join(output_dir, 'file_metadata.csv'), index=False)\n",
        "\n",
        "    print(f\"Features saved to: {output_dir}\")\n",
        "\n",
        "    # Display feature statistics\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FEATURE EXTRACTION SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Total files processed: {len(file_metadata)}\")\n",
        "\n",
        "    for model_name, features_array in features_dict.items():\n",
        "        print(f\"\\n{model_name.upper()} Features:\")\n",
        "        print(f\"  Shape: {features_array.shape}\")\n",
        "        print(f\"  Mean: {np.mean(features_array):.4f}\")\n",
        "        print(f\"  Std: {np.std(features_array):.4f}\")\n",
        "        print(f\"  Min: {np.min(features_array):.4f}\")\n",
        "        print(f\"  Max: {np.max(features_array):.4f}\")\n",
        "\n",
        "    # Gender distribution\n",
        "    print(f\"\\nGender Distribution:\")\n",
        "    print(metadata_df['gender'].value_counts())\n",
        "\n",
        "    return features_dict, metadata_df\n"
      ],
      "metadata": {
        "id": "IuZ_eV6rwzHT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 5: Execute Feature Extraction\n",
        "# ==========================================\n",
        "\n",
        "# Run the feature extraction\n",
        "print(\"Starting feature extraction process...\")\n",
        "features, metadata = process_gender_dataset(dataset_path)\n",
        "\n",
        "if features is not None:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FEATURE EXTRACTION COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Display final results\n",
        "    print(\"Feature dimensions match paper specifications:\")\n",
        "    print(f\"✓ VGGish: {features['vggish'].shape[1]} dimensions (expected: 128)\")\n",
        "    print(f\"✓ YAMNet: {features['yamnet'].shape[1]} dimensions (expected: 1024)\")\n",
        "    print(f\"✓ PANNs: {features['panns'].shape[1]} dimensions (expected: 2048)\")\n",
        "\n",
        "    print(f\"\\nFeatures saved in './audio_features/' directory\")\n",
        "    print(\"Files created:\")\n",
        "    print(\"- vggish_features.npy\")\n",
        "    print(\"- yamnet_features.npy\")\n",
        "    print(\"- panns_features.npy\")\n",
        "    print(\"- file_metadata.csv\")\n",
        "\n",
        "else:\n",
        "    print(\"Feature extraction failed. Please check the dataset path and file formats.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJs3Xw_sw1kR",
        "outputId": "5cf5ea20-1f8b-499c-da1a-9bafcc20a811"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting feature extraction process...\n",
            "Initializing feature extractor...\n",
            "Loading pre-trained models...\n",
            "Loading VGGish...\n",
            "Loading YAMNet...\n",
            "Setting up PANNs...\n",
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n",
            "GPU number: 1\n",
            "PANNs loaded on cuda\n",
            "All models loaded successfully!\n",
            "Scanning for audio files...\n",
            "Found 16148 audio files\n",
            "Extracting features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing audio files: 100%|██████████| 16148/16148 [07:59<00:00, 33.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGGISH features shape: (16148, 128)\n",
            "YAMNET features shape: (16148, 1024)\n",
            "PANNS features shape: (16148, 2048)\n",
            "Saving extracted features...\n",
            "Features saved to: ./audio_features\n",
            "\n",
            "==================================================\n",
            "FEATURE EXTRACTION SUMMARY\n",
            "==================================================\n",
            "Total files processed: 16148\n",
            "\n",
            "VGGISH Features:\n",
            "  Shape: (16148, 128)\n",
            "  Mean: nan\n",
            "  Std: nan\n",
            "  Min: nan\n",
            "  Max: nan\n",
            "\n",
            "YAMNET Features:\n",
            "  Shape: (16148, 1024)\n",
            "  Mean: 0.1065\n",
            "  Std: 0.2188\n",
            "  Min: 0.0000\n",
            "  Max: 3.7249\n",
            "\n",
            "PANNS Features:\n",
            "  Shape: (16148, 2048)\n",
            "  Mean: 0.0942\n",
            "  Std: 0.2484\n",
            "  Min: 0.0000\n",
            "  Max: 4.3367\n",
            "\n",
            "Gender Distribution:\n",
            "gender\n",
            "unknown    16148\n",
            "Name: count, dtype: int64\n",
            "\n",
            "==================================================\n",
            "FEATURE EXTRACTION COMPLETED SUCCESSFULLY!\n",
            "==================================================\n",
            "Feature dimensions match paper specifications:\n",
            "✓ VGGish: 128 dimensions (expected: 128)\n",
            "✓ YAMNet: 1024 dimensions (expected: 1024)\n",
            "✓ PANNs: 2048 dimensions (expected: 2048)\n",
            "\n",
            "Features saved in './audio_features/' directory\n",
            "Files created:\n",
            "- vggish_features.npy\n",
            "- yamnet_features.npy\n",
            "- panns_features.npy\n",
            "- file_metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP 6: Feature Loading Function for Later Use\n",
        "# ==========================================\n",
        "\n",
        "'''def load_extracted_features(features_dir='./audio_features'):\n",
        "    \"\"\"Load previously extracted features\"\"\"\n",
        "    try:\n",
        "        features = {}\n",
        "        features['vggish'] = np.load(os.path.join(features_dir, 'vggish_features.npy'))\n",
        "        features['yamnet'] = np.load(os.path.join(features_dir, 'yamnet_features.npy'))\n",
        "        features['panns'] = np.load(os.path.join(features_dir, 'panns_features.npy'))\n",
        "\n",
        "        metadata = pd.read_csv(os.path.join(features_dir, 'file_metadata.csv'))\n",
        "\n",
        "        print(\"Features loaded successfully!\")\n",
        "        for model_name, feature_array in features.items():\n",
        "            print(f\"{model_name}: {feature_array.shape}\")\n",
        "\n",
        "        return features, metadata\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading features: {e}\")\n",
        "        return None, None\n",
        "'''\n",
        "\n",
        "# features, metadata = load_extracted_features()\n",
        "\n"
      ],
      "metadata": {
        "id": "qh_UECeSw5Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Inspect the VGGish features array\n",
        "\n",
        "print(\"VGGish Features Shape:\", vggish_features.shape)\n",
        "print(\"First 5 VGGish Features:\")\n",
        "print(vggish_features[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TM2sSXb1Bxd",
        "outputId": "6cbd6f72-f43d-4127-f2cd-f3b50f5b6772"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGGish Features Shape: (16148, 128)\n",
            "First 5 VGGish Features:\n",
            "[[-1.17662460e-01  1.06528535e-01 -1.36822879e-01 -5.79937458e-01\n",
            "   1.45776987e-01 -2.72055119e-01 -8.25421095e-01  3.71271521e-01\n",
            "  -7.66780555e-01 -6.77676082e-01 -8.69919896e-01 -2.54069090e-01\n",
            "  -1.39017797e+00 -5.68271041e-01 -2.37168193e-01 -1.27535596e-01\n",
            "  -3.36427718e-01  3.64160687e-01 -1.92880377e-01 -2.03614026e-01\n",
            "   8.58277231e-02  1.40275657e-02 -2.23799914e-01  2.58785486e-01\n",
            "   1.00702167e-01 -1.85411483e-01 -2.61564255e-02  6.81840360e-01\n",
            "  -3.82189274e-01 -4.53151584e-01 -2.65098423e-01 -6.34601116e-02\n",
            "  -3.90202016e-01  1.55591175e-01  5.95691442e-01 -3.30925405e-01\n",
            "  -8.25355828e-01 -3.99493039e-01 -8.43472540e-01 -6.35698974e-01\n",
            "   1.82024091e-01 -4.86520588e-01 -1.80906072e-01 -5.20078957e-01\n",
            "   4.43429381e-01 -2.46865749e-02  3.82902056e-01  3.74834239e-02\n",
            "  -3.42325866e-02  2.03785226e-01  4.10499275e-01 -5.24551034e-01\n",
            "   2.63129592e-01 -1.23950016e+00  1.49423003e-01 -5.65248847e-01\n",
            "   5.71267009e-02 -1.09674633e+00 -3.02662939e-01 -2.65176743e-01\n",
            "  -1.27374798e-01 -4.69260931e-01 -3.61451685e-01 -5.44999778e-01\n",
            "  -5.03634274e-01 -2.25041404e-01 -5.16323447e-01  5.18130243e-01\n",
            "  -2.93504894e-01  2.12812215e-01 -2.55238116e-01  1.71426952e-01\n",
            "  -1.26491737e+00 -2.42012471e-01 -2.98649490e-01 -5.07201076e-01\n",
            "  -6.62224948e-01  3.40332180e-01  1.31174251e-01  3.10902029e-01\n",
            "   5.75153753e-02  1.16856366e-01 -1.28446817e+00 -3.21875364e-01\n",
            "   8.46004605e-01 -1.88032895e-01 -2.98206776e-01  1.55686438e-01\n",
            "  -3.67447436e-01 -1.72801197e-01 -1.35082871e-01  3.52815300e-01\n",
            "  -6.87526882e-01  1.26640964e+00  7.19490647e-02  3.29979479e-01\n",
            "  -7.24703968e-02 -7.74136126e-01 -7.97699094e-01 -2.53164291e-01\n",
            "  -2.12955430e-01  3.00367832e-01  3.37948911e-02  8.91965866e-01\n",
            "   9.97126698e-01 -5.27358949e-01 -4.38874364e-02 -6.91356361e-01\n",
            "   2.29041234e-01 -1.81789204e-01 -5.39471090e-01 -3.43431979e-01\n",
            "  -8.09828863e-02 -6.97193801e-01 -6.43980742e-01 -2.64904767e-01\n",
            "  -2.85239905e-01  7.54639089e-01 -1.28804553e+00  1.62573278e-01\n",
            "  -4.22343254e-01 -1.88674390e-01 -4.43029761e-01 -5.03171623e-01\n",
            "   1.21458352e+00  6.13653183e-01 -2.51340419e-02 -3.80917251e-01]\n",
            " [ 1.95194364e-01  1.20221078e-03 -1.90821886e-01 -7.76372433e-01\n",
            "   3.20009589e-02 -2.58846283e-01 -8.62395525e-01  6.50213480e-01\n",
            "  -5.45191050e-01 -5.34803271e-01 -4.97771621e-01 -1.34758055e-01\n",
            "  -1.21509314e+00 -4.88127768e-01 -8.22280720e-02  7.14039877e-02\n",
            "  -2.04050153e-01  2.14651331e-01 -4.22080636e-01 -1.80656463e-01\n",
            "   1.37585998e-01  1.79894194e-01 -3.70973945e-01  5.46819150e-01\n",
            "   2.17258275e-01 -3.63770485e-01 -8.43579024e-02  6.74582958e-01\n",
            "  -5.11967838e-01 -6.11698925e-01 -5.42151451e-01  9.06759948e-02\n",
            "  -4.54610676e-01  1.14276975e-01  1.55703574e-01 -3.18731278e-01\n",
            "  -7.54444540e-01 -6.36067867e-01 -1.04452050e+00 -7.05525637e-01\n",
            "   2.47492924e-01 -6.21173799e-01 -3.36563528e-01 -5.14559388e-01\n",
            "   1.42311305e-01 -2.14769050e-01  2.47141123e-01  1.68239743e-01\n",
            "  -1.06711298e-01  1.95568174e-01  3.79273176e-01 -6.35430157e-01\n",
            "   1.55318320e-01 -9.42474425e-01  2.02435911e-01 -7.04210639e-01\n",
            "   6.59450591e-02 -8.77697825e-01 -3.07483763e-01 -4.71268505e-01\n",
            "  -7.09279776e-02 -5.11381984e-01 -2.01961562e-01 -8.92871022e-01\n",
            "  -6.98052406e-01 -2.30342954e-01 -4.93985534e-01  1.33571401e-01\n",
            "  -2.66058922e-01  1.67097077e-02 -2.56969213e-01  9.11628455e-02\n",
            "  -9.51177478e-01 -2.66178548e-01 -3.81470859e-01 -3.53290617e-01\n",
            "  -5.79993367e-01  3.55884016e-01  7.25280195e-02  2.91213274e-01\n",
            "  -5.17729707e-02  8.51220563e-02 -1.17862678e+00 -2.20404208e-01\n",
            "   8.03564489e-01 -3.45616162e-01 -3.69645894e-01 -1.16349481e-01\n",
            "  -3.15093935e-01 -8.08662623e-02 -4.08359587e-01  1.21108547e-01\n",
            "  -6.72838569e-01  1.41970563e+00  7.08805025e-02  3.64841998e-01\n",
            "  -1.28031373e-01 -5.41044950e-01 -9.35341656e-01 -2.46089786e-01\n",
            "  -1.05426803e-01  4.29766953e-01 -1.14211969e-01  9.19464171e-01\n",
            "   8.75669956e-01 -7.06816554e-01  3.38699594e-02 -6.72053874e-01\n",
            "  -3.88995260e-02 -1.93033159e-01 -6.73446476e-01 -2.44418487e-01\n",
            "   1.55336857e-01 -5.81807315e-01 -5.17173290e-01 -3.94615263e-01\n",
            "  -5.10164618e-01  1.18546508e-01 -9.38636601e-01  4.51168776e-01\n",
            "  -4.39651698e-01 -4.91468050e-02 -4.80294883e-01 -3.66174191e-01\n",
            "   1.91543794e+00  6.59098983e-01 -1.30871180e-02 -4.23062325e-01]\n",
            " [ 9.01414379e-02 -4.11485769e-02 -7.90605918e-02 -8.95324051e-01\n",
            "   3.05943340e-01 -3.32237154e-01 -8.55490923e-01  8.01747665e-02\n",
            "  -1.15505993e+00 -7.53274739e-01 -7.70787656e-01 -3.13606590e-01\n",
            "  -1.51748788e+00 -4.48324531e-01 -2.94094414e-01  3.63137007e-01\n",
            "  -4.60760951e-01  3.61114651e-01 -4.62762356e-01 -6.78002834e-02\n",
            "   4.66028340e-02  1.30620584e-01 -4.61321235e-01  3.07743222e-01\n",
            "   3.49495262e-01 -1.07344478e-01  2.40712357e-03  5.66339195e-01\n",
            "  -6.90290689e-01 -5.61387002e-01 -5.09331226e-01  8.80404487e-02\n",
            "  -5.27740479e-01  2.34679505e-01  4.73286718e-01 -4.31583494e-01\n",
            "  -8.82396281e-01 -5.97758055e-01 -1.21158218e+00 -6.37480199e-01\n",
            "   3.84259701e-01 -8.68889391e-01 -2.67460495e-01 -5.74544370e-01\n",
            "   2.72434920e-01 -4.14387286e-02  4.82473463e-01  1.16036378e-01\n",
            "   1.37120426e-01  1.76388979e-01  2.04301938e-01 -8.62820208e-01\n",
            "  -7.23760650e-02 -1.18761587e+00  4.12941307e-01 -8.27360868e-01\n",
            "   4.34474237e-02 -1.15123117e+00 -3.01258713e-01 -3.64648968e-01\n",
            "  -9.67977121e-02 -4.25188780e-01 -1.37824640e-01 -6.82512343e-01\n",
            "  -5.16070366e-01 -1.83737695e-01 -5.41916847e-01  4.60256189e-01\n",
            "  -3.56927067e-01  1.61289603e-01 -2.18939170e-01  3.66003722e-01\n",
            "  -1.01040375e+00 -9.82976258e-02 -2.98705906e-01 -4.37983423e-01\n",
            "  -8.79316330e-01  5.76845407e-01 -3.71930189e-02  3.81249070e-01\n",
            "  -2.40300491e-01  1.83870479e-01 -1.43662035e+00 -1.24217004e-01\n",
            "   6.77534103e-01 -3.13111335e-01 -4.34851974e-01  2.90617257e-01\n",
            "  -5.06493509e-01 -1.14025913e-01 -2.99786001e-01  3.52903098e-01\n",
            "  -7.34946251e-01  1.04448378e+00  2.00964287e-01  4.92454052e-01\n",
            "  -8.42114389e-02 -9.25517082e-01 -1.06696665e+00 -1.92837194e-01\n",
            "  -3.25087279e-01  3.61990333e-01 -8.00306126e-02  9.42901134e-01\n",
            "   7.58041203e-01 -5.02959251e-01 -1.08679451e-01 -9.53527629e-01\n",
            "   2.10262346e-03 -1.75423846e-01 -6.32734716e-01 -1.80495515e-01\n",
            "  -3.62193845e-02 -8.85868788e-01 -7.88977683e-01 -3.65749329e-01\n",
            "  -5.12495160e-01  2.75662422e-01 -1.25497949e+00  3.81172568e-01\n",
            "  -4.01488990e-01 -5.30175120e-02 -4.86247063e-01 -2.07693592e-01\n",
            "   9.23956871e-01  3.05959076e-01 -5.81472330e-02 -2.68056154e-01]\n",
            " [ 2.07483411e-01  2.75400877e-02 -2.14117374e-02 -7.57784605e-01\n",
            "   1.14485547e-02 -2.87207872e-01 -6.61815226e-01  2.67883569e-01\n",
            "  -9.31429625e-01 -4.86584812e-01 -7.14662254e-01 -3.35011244e-01\n",
            "  -1.31519711e+00 -4.13946122e-01 -1.86520219e-01  1.16961330e-01\n",
            "  -2.53299564e-01  2.57621795e-01 -1.39664829e-01  3.16941626e-02\n",
            "   4.66274142e-01  1.77311853e-01 -3.27728719e-01  2.95196176e-01\n",
            "   1.28972337e-01 -1.66607514e-01  1.28577292e-01  4.97053504e-01\n",
            "  -4.83621359e-01 -3.27336758e-01 -2.84721762e-01 -5.30716293e-02\n",
            "  -3.13180923e-01  2.06632853e-01  1.75620332e-01 -2.71878779e-01\n",
            "  -8.49524796e-01 -3.41518283e-01 -9.37730968e-01 -5.01508892e-01\n",
            "   5.24853408e-01 -6.57578886e-01 -1.92571983e-01 -4.63395953e-01\n",
            "   1.24568127e-01  6.43040836e-02  3.61220568e-01  1.66482121e-01\n",
            "   1.84584931e-01  2.67323107e-01  9.96736586e-02 -7.08145440e-01\n",
            "  -5.64791299e-02 -9.61377323e-01  4.54851300e-01 -7.13324547e-01\n",
            "   3.48047502e-02 -9.48456705e-01 -1.53922066e-01 -1.50554463e-01\n",
            "   9.27456021e-02 -4.07665759e-01 -1.89310685e-01 -4.42009926e-01\n",
            "  -5.13044834e-01 -1.94848225e-01 -4.71198767e-01  3.46115023e-01\n",
            "  -2.62652665e-01  2.10838333e-01 -1.17914878e-01  5.62212288e-01\n",
            "  -1.17751372e+00  3.40969749e-02 -3.69937986e-01 -3.32992226e-01\n",
            "  -6.04239941e-01  4.55997467e-01 -8.55957046e-02  2.35884979e-01\n",
            "   1.32612912e-02  1.16395615e-01 -1.14466619e+00 -1.46914974e-01\n",
            "   7.94431686e-01 -3.09075534e-01 -3.39146823e-01 -6.64731711e-02\n",
            "  -3.60665411e-01 -1.55341879e-01 -3.68292570e-01  4.85333204e-01\n",
            "  -4.31910008e-01  9.02037144e-01  2.46743441e-01  2.54202276e-01\n",
            "   6.44876584e-02 -7.12190211e-01 -1.02294624e+00 -2.25307107e-01\n",
            "  -2.54216194e-01  3.75264019e-01 -1.31440341e-01  8.80841434e-01\n",
            "   7.38960743e-01 -3.04158717e-01 -6.28559813e-02 -6.26007736e-01\n",
            "  -5.22413068e-02 -7.91801214e-02 -5.48445404e-01 -2.40139678e-01\n",
            "  -1.70319583e-02 -6.10023320e-01 -5.79096735e-01 -3.66213888e-01\n",
            "  -5.14301479e-01  1.00173719e-01 -1.17071056e+00  5.86718857e-01\n",
            "  -2.63795733e-01 -5.07793576e-02 -3.70697528e-01 -1.93180397e-01\n",
            "   1.01592433e+00  3.70609283e-01 -1.85670927e-01 -4.03922558e-01]\n",
            " [-7.45855048e-02  7.47788921e-02 -6.45758584e-02 -1.20903087e+00\n",
            "  -9.31499451e-02 -1.87729388e-01 -9.38652754e-01  2.11187452e-01\n",
            "  -1.21561217e+00 -8.06542337e-01 -6.17529631e-01 -4.61469471e-01\n",
            "  -1.86524916e+00 -5.47361970e-01 -1.21804357e-01  1.54355735e-01\n",
            "  -3.46417129e-01  2.97026545e-01 -4.81103122e-01  1.04025118e-02\n",
            "   2.23870814e-01  3.69180381e-01 -5.03956139e-01  3.00084054e-01\n",
            "   3.20716739e-01 -1.72716767e-01  3.11521515e-02  4.81072247e-01\n",
            "  -8.26961398e-01 -5.50076187e-01 -4.38373744e-01 -6.30561337e-02\n",
            "  -5.40044188e-01  6.00167662e-02  2.74745636e-02 -5.73384643e-01\n",
            "  -9.84248221e-01 -9.22347903e-01 -1.23108518e+00 -6.46383762e-01\n",
            "   5.97917378e-01 -1.00645399e+00 -5.42051077e-01 -5.86752057e-01\n",
            "   3.08018565e-01  4.83313203e-02  1.94471568e-01  1.14522353e-01\n",
            "   8.04239064e-02  2.23850220e-01  1.68814018e-01 -8.69128823e-01\n",
            "   2.76813880e-02 -1.31574070e+00  2.12633714e-01 -9.57321644e-01\n",
            "   1.48681328e-02 -1.27771366e+00 -3.65398169e-01 -3.16426069e-01\n",
            "  -2.06643596e-01 -6.16588116e-01 -1.58682048e-01 -8.82070541e-01\n",
            "  -7.76221752e-01 -2.12359488e-01 -6.32757664e-01  3.51009130e-01\n",
            "  -2.85113573e-01  8.57823044e-02 -2.23914236e-01  3.28809500e-01\n",
            "  -1.39815116e+00 -7.87320584e-02 -5.05248427e-01 -6.43761992e-01\n",
            "  -7.66663313e-01  2.45896459e-01 -2.22256929e-01  2.17014477e-01\n",
            "  -9.31674093e-02  6.18458018e-02 -1.57867718e+00 -7.66113847e-02\n",
            "   9.72384453e-01 -2.97936440e-01 -4.71462220e-01 -2.53844947e-01\n",
            "  -3.71715456e-01 -1.55241475e-01 -2.26113975e-01  4.69181389e-01\n",
            "  -7.22492754e-01  1.51829267e+00  1.09618396e-01  1.42492324e-01\n",
            "  -4.24444467e-01 -7.81114519e-01 -1.17872763e+00 -2.19484419e-01\n",
            "  -3.35301220e-01  6.56989872e-01 -2.57925034e-01  8.13563585e-01\n",
            "   1.33471060e+00 -4.62451935e-01 -2.26581246e-01 -1.08181643e+00\n",
            "  -1.05697922e-02  1.39813080e-01 -7.57928967e-01 -3.95328611e-01\n",
            "  -2.23648623e-02 -8.15640748e-01 -7.23823607e-01 -4.42754835e-01\n",
            "  -4.97515649e-01  1.59591779e-01 -1.13819897e+00  6.40248895e-01\n",
            "  -6.11829877e-01 -3.45132947e-02 -4.35384274e-01 -5.91065764e-01\n",
            "   1.08654952e+00  3.82062376e-01 -9.26342010e-02 -4.09583271e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for NaN values in VGGish features\n",
        "nan_in_vggish = np.isnan(features['vggish']).any()\n",
        "print(f\"Are there any NaN values in VGGish features? {nan_in_vggish}\")\n",
        "\n",
        "if nan_in_vggish:\n",
        "    nan_rows_vggish = np.isnan(features['vggish']).any(axis=1)\n",
        "    num_nan_rows_vggish = np.sum(nan_rows_vggish)\n",
        "    print(f\"Number of rows with NaN values in VGGish features: {num_nan_rows_vggish}\")\n",
        "\n",
        "    # You can also find the indices of these rows\n",
        "    nan_row_indices_vggish = np.where(nan_rows_vggish)[0]\n",
        "    print(f\"Indices of rows with NaN values (first 10): {nan_row_indices_vggish[:10]}\")\n",
        "\n",
        "    # To see the metadata for these files (assuming metadata is indexed the same way)\n",
        "    if metadata is not None:\n",
        "        print(\"\\nMetadata for first 10 files with NaN in VGGish features:\")\n",
        "        display(metadata.iloc[nan_row_indices_vggish[:10]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "zK8EIQmZ2Upy",
        "outputId": "c155cdec-1b5b-41eb-9ada-9c1bc7c94119"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are there any NaN values in VGGish features? True\n",
            "Number of rows with NaN values in VGGish features: 4\n",
            "Indices of rows with NaN values (first 10): [ 2214  7618 11722 14702]\n",
            "\n",
            "Metadata for first 10 files with NaN in VGGish features:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       index             filename  \\\n",
              "2214    2214  arctic_a0207(4).wav   \n",
              "7618    7618  arctic_a0207(1).wav   \n",
              "11722  11722  arctic_a0329(3).wav   \n",
              "14702  14702  arctic_a0542(6).wav   \n",
              "\n",
              "                                               file_path   gender  \n",
              "2214   /kaggle/input/gender-recognition-by-voiceorigi...  unknown  \n",
              "7618   /kaggle/input/gender-recognition-by-voiceorigi...  unknown  \n",
              "11722  /kaggle/input/gender-recognition-by-voiceorigi...  unknown  \n",
              "14702  /kaggle/input/gender-recognition-by-voiceorigi...  unknown  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f76da3a1-45f9-48e5-9a01-d46d46f82021\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>filename</th>\n",
              "      <th>file_path</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2214</th>\n",
              "      <td>2214</td>\n",
              "      <td>arctic_a0207(4).wav</td>\n",
              "      <td>/kaggle/input/gender-recognition-by-voiceorigi...</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7618</th>\n",
              "      <td>7618</td>\n",
              "      <td>arctic_a0207(1).wav</td>\n",
              "      <td>/kaggle/input/gender-recognition-by-voiceorigi...</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11722</th>\n",
              "      <td>11722</td>\n",
              "      <td>arctic_a0329(3).wav</td>\n",
              "      <td>/kaggle/input/gender-recognition-by-voiceorigi...</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14702</th>\n",
              "      <td>14702</td>\n",
              "      <td>arctic_a0542(6).wav</td>\n",
              "      <td>/kaggle/input/gender-recognition-by-voiceorigi...</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f76da3a1-45f9-48e5-9a01-d46d46f82021')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f76da3a1-45f9-48e5-9a01-d46d46f82021 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f76da3a1-45f9-48e5-9a01-d46d46f82021');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ee8ee276-e8ba-4d3c-a1b0-12579baa9c05\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee8ee276-e8ba-4d3c-a1b0-12579baa9c05')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ee8ee276-e8ba-4d3c-a1b0-12579baa9c05 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"        display(metadata\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"index\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5411,\n        \"min\": 2214,\n        \"max\": 14702,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          7618,\n          14702,\n          2214\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"arctic_a0207(1).wav\",\n          \"arctic_a0542(6).wav\",\n          \"arctic_a0207(4).wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"/kaggle/input/gender-recognition-by-voiceoriginal/data/male/arctic_a0207(1).wav\",\n          \"/kaggle/input/gender-recognition-by-voiceoriginal/data/male/arctic_a0542(6).wav\",\n          \"/kaggle/input/gender-recognition-by-voiceoriginal/data/female/arctic_a0207(4).wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"unknown\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}